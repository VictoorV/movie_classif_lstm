{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictoorV/movie_classif_lstm/blob/main/Film_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext==0.15.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BlrgZFCLyD18",
        "outputId": "3658aea1-237e-4a6b-f9c8-6c4b144f7e13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchtext==0.15.2)\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (3.31.4)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
            "Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbS-jOpcGjuT",
        "outputId": "0db39f2a-dd60-4a1c-b60a-8c2b08348944"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hy5sOtPEpwJC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "Kxf41O9wGNHt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = IMDB()\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocabulary = build_vocab_from_iterator(\n",
        "    yield_tokens(train_iter),\n",
        "    specials=[\"<pad>\", \"<unk>\"])\n",
        "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
        "\n",
        "print(len(vocabulary))\n",
        "print(vocabulary['<pad>'])\n",
        "print(vocabulary.get_itos()[0])"
      ],
      "metadata": {
        "id": "SGGhW7gnEi4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756ad238-e953-4d71-f989-644fc1c63c9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100684\n",
            "0\n",
            "<pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "print(f'Taille du train_dataset : {len(train_dataset)}')\n",
        "print(f'Taille du test_dataset : {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "gVa1Mm2R37r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a37882f-bba9-4bad-8c37-f2863ddcf7e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du train_dataset : 25000\n",
            "Taille du test_dataset : 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sort = True\n",
        "\n",
        "def collate_batch(batch, sort = sort):\n",
        "  labels, samples = zip(*batch)\n",
        "  labels = torch.tensor(labels, dtype=torch.int64) - 1\n",
        "  text_lengths = torch.tensor([len(tokenizer(sample)) for sample in samples], dtype=torch.int64)\n",
        "  inputs = [torch.tensor(vocabulary(tokenizer(sample)), dtype=torch.int64) for sample in samples]\n",
        "  inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "  if sort:\n",
        "    text_lengths, indices = torch.sort(text_lengths, descending=True)\n",
        "    labels = labels[indices]\n",
        "    inputs = inputs[indices]\n",
        "  return labels, inputs, text_lengths"
      ],
      "metadata": {
        "id": "Pu2d0_KN2wRW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "TD2FOx88MXZu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embedding_size,\n",
        "                 hidden_size,\n",
        "                 num_classes,\n",
        "                 dropout,\n",
        "                 num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding field\n",
        "        self.embedding = torch.nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_size)\n",
        "\n",
        "        # LSTM cell\n",
        "        self.rnn = torch.nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size,\n",
        "            bidirectional=True,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True)\n",
        "\n",
        "        # Fully connected output\n",
        "        self.fc = torch.nn.Linear(\n",
        "            2*hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, text_tokens, text_lengths):\n",
        "        # Extract embedding vectors\n",
        "        embeddings = self.embedding(\n",
        "            text_tokens)\n",
        "        packed = pack_padded_sequence(embeddings, text_lengths, batch_first=True, enforce_sorted=sort)\n",
        "        h_t, (h_n, c_n) = self.rnn(packed)\n",
        "\n",
        "        return self.fc(torch.cat((h_n[-2],h_n[-1]),dim=1))"
      ],
      "metadata": {
        "id": "Kk2K8xLbj3S5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, cost_function, optimizer, data_loader, scheduler=None):\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    current_loss = 0.0\n",
        "    current_acc = 0.0\n",
        "\n",
        "    for i, (labels, inputs, text_lengths) in enumerate(data_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs, text_lengths)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        loss = cost_function(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss += loss.item() * labels.size(0)\n",
        "        current_acc += torch.sum(predictions == labels.data)\n",
        "\n",
        "    total_loss = current_loss / len(data_loader.dataset)\n",
        "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "\n",
        "    print('Train Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "def test_model(model, cost_function, data_loader):\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    current_loss = 0.0\n",
        "    current_acc = 0.0\n",
        "\n",
        "    for i, (labels, inputs, text_lengths) in enumerate(data_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, text_lengths)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            loss = cost_function(outputs, labels)\n",
        "\n",
        "        current_loss += loss.item() * labels.size(0)\n",
        "        current_acc += torch.sum(predictions == labels.data)\n",
        "\n",
        "    total_loss = current_loss / len(data_loader.dataset)\n",
        "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "\n",
        "    print('Test Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))"
      ],
      "metadata": {
        "id": "b-qdA699isP8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(\n",
        "    vocab_size=len(vocabulary),\n",
        "    embedding_size=100,\n",
        "    hidden_size=64,\n",
        "    num_classes=2,\n",
        "    num_layers=2,\n",
        "    dropout=0.3)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "cost_fn = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "num_epoch = 8\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=num_epoch, eta_min=1e-5)"
      ],
      "metadata": {
        "id": "OB8q2HZTkGwT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epoch):\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    train_model(model, cost_fn, optim, train_dataloader, scheduler=scheduler)\n",
        "    test_model(model, cost_fn, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__NeZOIZg8tL",
        "outputId": "bac7da96-1835-430f-e917-3ee199f29120"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.5868; Accuracy: 0.6786\n",
            "Test Loss: 0.5074; Accuracy: 0.7530\n",
            "Epoch: 2\n",
            "Train Loss: 0.5460; Accuracy: 0.7210\n",
            "Test Loss: 0.4765; Accuracy: 0.7856\n",
            "Epoch: 3\n",
            "Train Loss: 0.3996; Accuracy: 0.8198\n",
            "Test Loss: 0.3985; Accuracy: 0.8257\n",
            "Epoch: 4\n",
            "Train Loss: 0.2956; Accuracy: 0.8784\n",
            "Test Loss: 0.3596; Accuracy: 0.8489\n",
            "Epoch: 5\n",
            "Train Loss: 0.2183; Accuracy: 0.9144\n",
            "Test Loss: 0.3241; Accuracy: 0.8687\n",
            "Epoch: 6\n",
            "Train Loss: 0.1766; Accuracy: 0.9342\n",
            "Test Loss: 0.3299; Accuracy: 0.8720\n",
            "Epoch: 7\n",
            "Train Loss: 0.1527; Accuracy: 0.9450\n",
            "Test Loss: 0.3516; Accuracy: 0.8625\n",
            "Epoch: 8\n",
            "Train Loss: 0.1385; Accuracy: 0.9513\n",
            "Test Loss: 0.3395; Accuracy: 0.8733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "    (2, \"This movie was really great!\"),\n",
        "    (1, \"I am not sure about the scenario, the movie was empty and boring.\"),\n",
        "    (1, \"The movie was shit and I do not recommend it.\"),\n",
        "    (2, \"The actors were good, it was satisfying.\")\n",
        "]\n",
        "\n",
        "labels, inputs, text_lengths = collate_batch(sequences, sort)\n",
        "print(text_lengths)\n",
        "print(torch.max(model(inputs.to(device), text_lengths),1)[1])"
      ],
      "metadata": {
        "id": "qQ74xRnrSEON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60d5049-b032-46b6-9ebc-0063805325e4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([15, 11,  9,  6])\n",
            "tensor([0, 0, 1, 1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB1a3Nh-aHRc",
        "outputId": "88617471-dfd5-4811-f4ed-fb816de47606"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}